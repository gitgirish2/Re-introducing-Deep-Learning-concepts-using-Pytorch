{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "optim_pytorch",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSE3qaSAKyNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn6FqCcKK2Ap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kobHaQbWPE4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Constructing an Optimizer\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum=0.9)\n",
        "optimizer = optim.Adam([var1, var2], lr = 0.0001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs7638inPbL3",
        "colab_type": "text"
      },
      "source": [
        "To specify this, we are calling the optimizer that we require, and assign it an iterable containing the parameters (**Variable**s ) to optimize, the specifying the optimizer specific options such as learning rate, momentum, weight decay etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cR0a-DNFP8H5",
        "colab_type": "text"
      },
      "source": [
        "In order to update the parameter, we will use the step() method. This is done in two ways - optimizer.step() \n",
        "This is a simplified version supported by most optimizers. The function can be called once the gradients are computed using backward().\n",
        "\n",
        "\n",
        "and optimizer.step(closure)\n",
        "Some optimization algorithms such as Conjugate Gradient and LBFGS need to reevaluate the function multiple times, so you have to pass in a closure that allows them to recompute your model. The closure should clear the gradients, compute the loss, and return it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "376EgBjeQqnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimizer.step()\n",
        "for input, target in dataset:\n",
        "    optimizer.zero_grad()\n",
        "    output = model(input)\n",
        "    loss = loss_fn(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8hTZ3WAQ51J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimizer.step(closure)\n",
        "for input, target in dataset:\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        output = model(input)\n",
        "        loss = loss_fn(output, target)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "    optimizer.step(closure)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}