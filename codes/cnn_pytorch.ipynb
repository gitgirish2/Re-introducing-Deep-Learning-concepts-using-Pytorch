{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_pytorch.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "m5qOiuCadDdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pNgna05dWyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing necessary packages\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3LnOS1fduuh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing transforms and datasets from torchvision\n",
        "from torchvision.transforms import transforms\n",
        "import torchvision.datasets as datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQB6ubRpd_NR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using the MNIST dataset present in torchvision\n",
        "#obtain the training data with this syntax, check dataloaders\n",
        "train_data = datasets.MNIST(root='./data', \n",
        "                            train='True',\n",
        "                            transform=transforms.ToTensor(),\n",
        "                            download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fT9RLwPe8Lm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#obtain the testing data with this syntax, check dataloaders\n",
        "test_data = datasets.MNIST(root='./data',\n",
        "                           train=False,\n",
        "                           transform=transforms.ToTensor())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAXWw7-KfYYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#checking out the size of the training data\n",
        "print(train_data.train_data.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plBSwoNPgkIf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#checking out the size of the testing data\n",
        "print(test_data.test_data.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GL_I3NxkhOwN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#making dataset iterable\n",
        "batch_size = 200\n",
        "iterations = 4000\n",
        "epochs = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jT_VyvthaHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#working with the dataloader\n",
        "#creating the training data loader\n",
        "train_data_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                                batch_size=batch_size,\n",
        "                                                shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMp1tScbh7C-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating the testing data loader\n",
        "test_data_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                               batch_size=batch_size,\n",
        "                                               shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JW2JG-2ibuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create CNN model class\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    \n",
        "    #Convolution layer 1 using 2d convolution (hidden layer)\n",
        "    #considered image size = 28*28\n",
        "    #feature detecters = 16\n",
        "    #filters = 5; each of size 5*5 (kernel_size)\n",
        "    #stride=1\n",
        "    #padding=2\n",
        "    self.cnn_conv1 = nn.Conv2d(in_channels=1,\n",
        "                               out_channels=16,\n",
        "                               kernel_size=5,\n",
        "                               stride=1,\n",
        "                               padding=2)\n",
        "    #activation fuction\n",
        "    #will keep either x or 0, discards negative values\n",
        "    #majorly used as it yeilds faster training for larger neural networks\n",
        "    self.relu1 = nn.ReLU()\n",
        "    \n",
        "    #maxpooling layer 1; 2*2 pooling feature\n",
        "    self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
        "    \n",
        "    #Convolution layer 2 using 2d convolution (hidden layer)\n",
        "    #after pooling on 16 feature detecters, we have 32 feature detectors\n",
        "    #filter = 5; each of size 5*5 (kernel_size)\n",
        "    #stride=1\n",
        "    #padding=2\n",
        "    self.cnn_conv2 = nn.Conv2d(in_channels=16,\n",
        "                               out_channels=32,\n",
        "                               kernel_size=5,\n",
        "                               stride=1,\n",
        "                               padding=2)\n",
        "    #activation fuction\n",
        "    #will keep either x or 0, discards negative values\n",
        "    self.relu2 = nn.ReLU()\n",
        "    \n",
        "    #maxpooling layer 2; 2*2 pooling feature\n",
        "    self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
        "    \n",
        "    #for flattened images\n",
        "    #recall linear regression\n",
        "    #32 feature_map from conv layer 2\n",
        "    #then image size reduces due to pooling\n",
        "    #therefore, 32 pooled feature map each of 7*7\n",
        "    #find numbers between 0 to 9, 10 outputs\n",
        "    self.fc1 = nn.Linear(32*7*7, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYPRLZAwtdUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#forward pass\n",
        "def forward(self, x):\n",
        "  #Convolution1\n",
        "  out = self.cnn_conv1(x)\n",
        "  out = self.relu1(out)\n",
        "  \n",
        "  #Maxpool1\n",
        "  out = self.maxpool1(out)\n",
        "  \n",
        "  #c1  \n",
        "  out = self.cnn_conv2(x)\n",
        "  out = self.relu2(out)\n",
        "  \n",
        "  #Maxpool2\n",
        "  out = self.maxpool2(out)\n",
        "  \n",
        "  #Output to flattened images\n",
        "  out = out.view(out.size(0), -1)\n",
        "    \n",
        "  #Linear Function\n",
        "  out = self.fc1(out)\n",
        "  \n",
        "  return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYOVu6DLvoTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#optimizing the model\n",
        "model = CNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.01\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR6sVXY9w15q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training the model\n",
        "iterations = 0\n",
        "for epochs in range(epochs):\n",
        "  for i, (images,labels) in enumerate(train_data_loader):\n",
        "    #loading training data which has images and variables\n",
        "    images = Variable(images)\n",
        "    labels = Variable(labels)\n",
        "    \n",
        "    #clearning the  gradients\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    #passing the input into a model\n",
        "    output = model(images)\n",
        "    \n",
        "    #finding loss using ouput above\n",
        "    loss = criterion(output, labels)\n",
        "    \n",
        "    \n",
        "    #backward pass\n",
        "    loss.backward()\n",
        "    optimizser.step()\n",
        "    iterations = iterations + 1\n",
        "    if (i+1) % 100 == 0:\n",
        "      print('Epoch [%d/%d], Iterations [%d/%d] Loss: %.6f'\n",
        "            %(epoch+1, num_epochs, i+1, len(train_data)//batch_size, \n",
        "              loss.data[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWVRQr2Q2D4O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Testing the model\n",
        "model.eval()\n",
        "correct=0\n",
        "total=0\n",
        "for images, labels in test_data_loader:\n",
        "    #converting it into tensor\n",
        "    images = Variable(images)\n",
        "    #passing it to model\n",
        "    outputs = model(images)\n",
        "     #predictions\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "\n",
        "print('Test Accuracy of the model on the 10000 test images: %d %%' \n",
        "      % (100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}